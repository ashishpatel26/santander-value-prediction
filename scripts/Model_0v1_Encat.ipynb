{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0 Model: Encoding with CatBoost\n",
    "Inspired from: https://www.kaggle.com/tanreinama/catboost-stackedae-with-mxnet-meta-1-40lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "\n",
    "import time\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load h5py file\n",
    "def loadh5(fname, dname):\n",
    "    h5f = h5py.File(fname, 'r')\n",
    "    data = h5f[dname][:]\n",
    "    h5f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle file\n",
    "def loadpickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-wise metadata generator\n",
    "def generate_meta(df):\n",
    "    meta = pd.DataFrame({\n",
    "        'nonsparse_count': (df[df==0]).fillna(1).sum(axis=1),\n",
    "        'sum': df[df!=0].sum(axis=1),\n",
    "        'mean': df[df!=0].mean(axis=1),\n",
    "        'std': df[df!=0].std(axis=1),\n",
    "        'median': df[df!=0].median(axis=1),\n",
    "        'max': df[df!=0].max(axis=1),\n",
    "        'min': df[df!=0].min(axis=1),\n",
    "        'var': df[df!=0].var(axis=1)})\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage 0 vanilla train and test datasets...\n",
      "Loading completed in 1.53867983818 seconds\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "train_dname = 'train_s0'\n",
    "test_dname = 'test_s0'\n",
    "f_ext = '_vanilla.h5'\n",
    "\n",
    "load_start = time.time()\n",
    "print 'Loading Stage 0 vanilla train and test datasets...'\n",
    "# Load h5py data\n",
    "train_data = loadh5(data_path + train_dname + f_ext, train_dname)\n",
    "test_data = loadh5(data_path + test_dname + f_ext, test_dname)\n",
    "# Load dataframe indexes\n",
    "train_idx = loadpickle(data_path + 'train_idx.pkl')\n",
    "test_idx = loadpickle(data_path + 'test_idx.pkl')\n",
    "# Load dataframe column names\n",
    "train_cols = loadpickle(data_path + 'train_cols.pkl')\n",
    "test_cols = loadpickle(data_path + 'test_cols.pkl')\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.DataFrame(data=train_data, index=train_idx, columns=train_cols)\n",
    "test_df = pd.DataFrame(data=test_data, index=test_idx, columns=test_cols)\n",
    "\n",
    "print 'Loading completed in %s seconds'%(time.time()-load_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata data path\n",
    "meta_path = data_path + 'meta_stage0v1.csv'\n",
    "# Autoencoder data path\n",
    "auto_type = '155'\n",
    "autodir_path = './autoencoder/data/'\n",
    "autodata_path = autodir_path + 'data_' + auto_type + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format label values\n",
    "Y = np.log1p(train_df.target.values)\n",
    "train_df.drop(columns=['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master dataset and perform scaling \n",
    "X = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "scaled_X = X.div(X.max(), axis='columns')\n",
    "# Scale labels\n",
    "y_min = np.min(Y)\n",
    "y_max = np.max(Y)\n",
    "scaled_Y = (Y - y_min)/(y_max - y_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get autoencoder and metadata results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder data\n",
    "autodata = pd.read_csv(autodata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading row-wise metadata...\n",
      "Row-wise metadata loaded!\n"
     ]
    }
   ],
   "source": [
    "# Row-wise metadata\n",
    "if not os.path.exists(meta_path):\n",
    "    print 'Generating row-wise metadata...'\n",
    "    meta_X = generate_meta(scaled_X)\n",
    "    meta_X.to_csv(meta_path, index=False)\n",
    "    print 'Row-wise metadata generated!'\n",
    "else:\n",
    "    print 'Loading row-wise metadata...'\n",
    "    meta_X = pd.read_csv(meta_path)\n",
    "    print 'Row-wise metadata loaded!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=0\n",
    "num_clusters1 = 24\n",
    "num_clusters2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Mini-Batch KMeans...\n",
      "Running PCA...\n",
      "Running Truncated SVD...\n",
      "Running Gaussian Random Projection...\n",
      "Running Sparse Random Projection...\n"
     ]
    }
   ],
   "source": [
    "print 'Running Mini-Batch KMeans...'\n",
    "mbkm = MiniBatchKMeans(n_clusters=num_clusters2, random_state=random_state)\n",
    "mbkm_X = pd.DataFrame(mbkm.fit_transform(scaled_X))\n",
    "print 'Running PCA...'\n",
    "pca = PCA(n_components=num_clusters2, random_state=random_state)\n",
    "pca_X = pd.DataFrame(pca.fit_transform(scaled_X))\n",
    "print 'Running Truncated SVD...'\n",
    "tsvd = TruncatedSVD(n_components=num_clusters2, random_state=random_state)\n",
    "tsvd_X = pd.DataFrame(tsvd.fit_transform(scaled_X))\n",
    "print 'Running Gaussian Random Projection...'\n",
    "grp = GaussianRandomProjection(n_components=num_clusters1, eps=0.1, random_state=random_state)\n",
    "grp_X = pd.DataFrame(grp.fit_transform(scaled_X))\n",
    "print 'Running Sparse Random Projection...'\n",
    "srp = SparseRandomProjection(n_components=num_clusters1, dense_output=True, random_state=random_state)\n",
    "srp_X = pd.DataFrame(srp.fit_transform(scaled_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Catboost with Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all data\n",
    "X_all = pd.concat([scaled_X, meta_X, autodata, mbkm_X, pca_X, tsvd_X, grp_X, srp_X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Catboost model...\n",
      "0:\tlearn: 0.6003868\ttest: 0.6150948\tbest: 0.6150948 (0)\ttotal: 2.42s\tremaining: 20m 10s\n",
      "50:\tlearn: 0.2038517\ttest: 0.2069319\tbest: 0.2069319 (50)\ttotal: 1m 58s\tremaining: 17m 19s\n",
      "100:\tlearn: 0.1869580\ttest: 0.1907322\tbest: 0.1907322 (100)\ttotal: 3m 58s\tremaining: 15m 40s\n",
      "150:\tlearn: 0.1807887\ttest: 0.1879276\tbest: 0.1879276 (150)\ttotal: 6m 3s\tremaining: 14m 1s\n",
      "200:\tlearn: 0.1729352\ttest: 0.1862479\tbest: 0.1862479 (200)\ttotal: 8m 10s\tremaining: 12m 9s\n",
      "250:\tlearn: 0.1614323\ttest: 0.1851125\tbest: 0.1850644 (245)\ttotal: 10m 14s\tremaining: 10m 9s\n",
      "300:\tlearn: 0.1520621\ttest: 0.1845990\tbest: 0.1844701 (289)\ttotal: 12m 17s\tremaining: 8m 7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1844434794\n",
      "bestIteration = 308\n",
      "\n",
      "Shrink model to first 309 iterations.\n",
      "0:\tlearn: 0.6039946\ttest: 0.5985025\tbest: 0.5985025 (0)\ttotal: 2.33s\tremaining: 19m 21s\n",
      "50:\tlearn: 0.2011065\ttest: 0.2125550\tbest: 0.2125550 (50)\ttotal: 2m\tremaining: 17m 40s\n",
      "100:\tlearn: 0.1840978\ttest: 0.2019849\tbest: 0.2019849 (100)\ttotal: 3m 59s\tremaining: 15m 45s\n",
      "150:\tlearn: 0.1775792\ttest: 0.2001969\tbest: 0.2001832 (149)\ttotal: 5m 56s\tremaining: 13m 44s\n",
      "200:\tlearn: 0.1684848\ttest: 0.1995753\tbest: 0.1995074 (192)\ttotal: 7m 55s\tremaining: 11m 46s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1993238301\n",
      "bestIteration = 214\n",
      "\n",
      "Shrink model to first 215 iterations.\n",
      "0:\tlearn: 0.6030752\ttest: 0.6050223\tbest: 0.6050223 (0)\ttotal: 2.32s\tremaining: 19m 17s\n",
      "50:\tlearn: 0.2065250\ttest: 0.1990997\tbest: 0.1990997 (50)\ttotal: 1m 57s\tremaining: 17m 15s\n",
      "100:\tlearn: 0.1889546\ttest: 0.1836559\tbest: 0.1836559 (100)\ttotal: 3m 54s\tremaining: 15m 24s\n",
      "150:\tlearn: 0.1830486\ttest: 0.1810156\tbest: 0.1810156 (150)\ttotal: 5m 53s\tremaining: 13m 37s\n",
      "200:\tlearn: 0.1728922\ttest: 0.1783686\tbest: 0.1783686 (200)\ttotal: 7m 51s\tremaining: 11m 41s\n",
      "250:\tlearn: 0.1590076\ttest: 0.1779615\tbest: 0.1777534 (233)\ttotal: 9m 50s\tremaining: 9m 45s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1777533954\n",
      "bestIteration = 233\n",
      "\n",
      "Shrink model to first 234 iterations.\n",
      "0:\tlearn: 0.6041794\ttest: 0.5990821\tbest: 0.5990821 (0)\ttotal: 2.33s\tremaining: 19m 20s\n",
      "50:\tlearn: 0.2044245\ttest: 0.1997534\tbest: 0.1997534 (50)\ttotal: 1m 57s\tremaining: 17m 11s\n",
      "100:\tlearn: 0.1864780\ttest: 0.1867946\tbest: 0.1867946 (100)\ttotal: 3m 55s\tremaining: 15m 31s\n",
      "150:\tlearn: 0.1789924\ttest: 0.1848809\tbest: 0.1848602 (148)\ttotal: 5m 53s\tremaining: 13m 36s\n",
      "200:\tlearn: 0.1702636\ttest: 0.1843313\tbest: 0.1842797 (195)\ttotal: 7m 50s\tremaining: 11m 40s\n",
      "250:\tlearn: 0.1610553\ttest: 0.1838851\tbest: 0.1838672 (248)\ttotal: 9m 48s\tremaining: 9m 44s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1835841592\n",
      "bestIteration = 274\n",
      "\n",
      "Shrink model to first 275 iterations.\n",
      "0:\tlearn: 0.6048045\ttest: 0.5989833\tbest: 0.5989833 (0)\ttotal: 2.32s\tremaining: 19m 16s\n",
      "50:\tlearn: 0.2013420\ttest: 0.2167427\tbest: 0.2167427 (50)\ttotal: 1m 57s\tremaining: 17m 14s\n",
      "100:\tlearn: 0.1843090\ttest: 0.2045239\tbest: 0.2045239 (100)\ttotal: 3m 50s\tremaining: 15m 10s\n",
      "150:\tlearn: 0.1778049\ttest: 0.2020761\tbest: 0.2020668 (149)\ttotal: 5m 45s\tremaining: 13m 18s\n",
      "200:\tlearn: 0.1712850\ttest: 0.2003727\tbest: 0.2003727 (200)\ttotal: 7m 40s\tremaining: 11m 25s\n",
      "250:\tlearn: 0.1600512\ttest: 0.1992340\tbest: 0.1991517 (247)\ttotal: 9m 36s\tremaining: 9m 32s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1990457823\n",
      "bestIteration = 266\n",
      "\n",
      "Shrink model to first 267 iterations.\n"
     ]
    }
   ],
   "source": [
    "submissions = []\n",
    "print 'Training Catboost model...'\n",
    "for fold_id, (IDX_train, IDX_test) in enumerate(KFold(n_splits=5, random_state=random_state, shuffle=False).split(scaled_Y)):\n",
    "    # Partition training and test sets\n",
    "    X_train = X_all.iloc[IDX_train].values\n",
    "    X_test = X_all.iloc[IDX_test].values\n",
    "    Y_train = scaled_Y[IDX_train]\n",
    "    Y_test = scaled_Y[IDX_test]\n",
    "    \n",
    "    # Define Catboost model\n",
    "    cb_reg = CatBoostRegressor(iterations=500,\n",
    "                               learning_rate=0.05, \n",
    "                               depth=10,\n",
    "                               eval_metric='RMSE',\n",
    "                               random_seed=fold_id, \n",
    "                               bagging_temperature=0.2,\n",
    "                               od_type='Iter', \n",
    "                               metric_period=50,  # Rounds to process before calculating objective\n",
    "                               od_wait=20)\n",
    "    cb_reg.fit(X_train, Y_train, eval_set=(X_test, Y_test), cat_features=[], use_best_model=True, verbose=True)\n",
    "    target_pred = cb_reg.predict(X_all.iloc[scaled_Y.shape[0]:])\n",
    "    # Rescale target predictions and append to submissions list\n",
    "    target_pred = target_pred * (y_max - y_min) + y_min\n",
    "    submissions.append(np.expm1(target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format submissions\n",
    "mean_submissions = np.mean(submissions, axis=0)\n",
    "result = pd.DataFrame({'ID': test_idx, 'target': mean_submissions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions\n",
    "result_path = '../submissions/encat_0v1_submit.csv'\n",
    "result.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
